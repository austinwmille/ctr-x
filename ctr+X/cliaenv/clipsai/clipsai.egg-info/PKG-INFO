Metadata-Version: 2.1
Name: clipsai
Version: 0.2.1
Summary: Clips AI is an open-source Python library that automatically converts long videos into clips
Home-page: https://clipsai.com/
Author: Benjamin Smidt, Johann Ramirez, Armel Talla
Author-email: support@clipsai.com
License: MIT
Project-URL: Documentation, https://docs.clipsai.com/
Project-URL: Homepage, https://clipsai.com/
Project-URL: Repository, https://github.com/ClipsAI/clipsai
Project-URL: Issues, https://github.com/ClipsAI/clipsai/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: av
Requires-Dist: facenet-pytorch
Requires-Dist: matplotlib
Requires-Dist: mediapipe
Requires-Dist: nltk
Requires-Dist: numpy
Requires-Dist: opencv-python
Requires-Dist: pandas
Requires-Dist: psutil
Requires-Dist: pyannote.audio
Requires-Dist: pyannote.core
Requires-Dist: pynvml
Requires-Dist: pytest
Requires-Dist: python-magic
Requires-Dist: scenedetect
Requires-Dist: scikit-learn
Requires-Dist: sentence-transformers
Requires-Dist: scipy
Requires-Dist: torch
Provides-Extra: dev
Requires-Dist: black; extra == "dev"
Requires-Dist: black[jupyter]; extra == "dev"
Requires-Dist: build; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Requires-Dist: ipykernel; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Requires-Dist: twine; extra == "dev"

# ClipsAI

<!-- [![PyPI version](https://badge.fury.io/py/project-name.svg)](https://badge.fury.io/py/project-name) -->
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


## Quickstart

Clips AI is an open-source Python library that automatically converts long videos into
clips. With just a few lines of code, you can segment a video into multiple clips and
resize its aspect ratio from 16:9 to 9:16.

> **Note:** Clips AI is designed for audio-centric, narrative-based videos such as
podcasts, interviews, speeches, and sermons. It actively employs video transcripts to
identify and create clips. Our resizing algorithm dynamically reframes and focuses on
the current speaker, converting the video into various aspect ratios.

For full documentation, visit [Clips AI Documentation](https://clipsai.com).
Check out a [UI demo](https://demo.clipsai.com) with clips generated by this library.

### Installation

1. Install Python dependencies. <br></br> *We highly suggest using a virtual environment (such as [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/#create-and-use-virtual-environments)) to avoid dependency conflicts*
    ```bash {{ language: 'python' }}
    pip install clipsai
    ```

    ```bash {{ language: 'python' }}
    pip install whisperx@git+https://github.com/m-bain/whisperx.git
    ```

2. Install [libmagic](https://github.com/ahupp/python-magic?tab=readme-ov-file#debianubuntu)

3. Install [ffmpeg](https://github.com/kkroening/ffmpeg-python/tree/master?tab=readme-ov-file#installing-ffmpeg)

### Creating clips

Since clips are found using the video's transcript, the video must first be transcribed. Transcribing is done with [WhisperX](https://github.com/m-bain/whisperX), an open-source wrapper on [Whisper](https://github.com/openai/whisper) with additional functionality for detecting start and stop times for each word. For trimming the original video into a chosen clip, refer to the clipping reference.

```python
from clipsai import ClipFinder, Transcriber

transcriber = Transcriber()
transcription = transcriber.transcribe(audio_file_path="/abs/path/to/video.mp4")

clipfinder = ClipFinder()
clips = clipfinder.find_clips(transcription=transcription)

print("StartTime: ", clips[0].start_time)
print("EndTime: ", clips[0].end_time)
```

### Resizing a video

A hugging face access token is required to resize a video since [Pyannote](https://github.com/pyannote/pyannote-audio) is utilized for speaker diarization. You won't be charged for using Pyannote and instructions are on the [Pyannote HuggingFace ](https://huggingface.co/pyannote/speaker-diarization-3.0#requirements) page. For resizing the original video to the desired aspect ratio, refer to the resizing reference.

```python
from clipsai import resize

crops = resize(
    video_file_path="/abs/path/to/video.mp4",
    pyannote_auth_token="pyannote_token",
    aspect_ratio=(9, 16)
)

print("Crops: ", crops.segments)
```
